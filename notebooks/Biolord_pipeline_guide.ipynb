{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Biolord Training and Prediction Pipeline\n",
        "\n",
        "This notebook contains:\n",
        "1. Required setup and packages\n",
        "2. Data preparation with GO features\n",
        "3. Training pipeline\n",
        "4. Prediction code verification\n",
        "5. Suggested prediction code based on findings\n",
        "\n",
        "Note: Paths and configurations need adjustment for HPC environment."
      ],
      "metadata": {
        "id": "KwWiLfkUqxyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required packages\n",
        "!pip install scanpy\n",
        "!pip install biolord\n",
        "import scanpy as sc\n",
        "import biolord\n",
        "import numpy as np\n",
        "import gc\n",
        "import os\n",
        "import torch\n",
        "import psutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clear memory\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Available RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")"
      ],
      "metadata": {
        "id": "_rB9dA-Nq6-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "Creating GO features and preparing data with perturbation attributes"
      ],
      "metadata": {
        "id": "05qXMtHHrLte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gene_id_mapping():\n",
        "    \"\"\"\n",
        "    Get mapping between gene symbols and NCBI gene IDs for human genes\n",
        "    \"\"\"\n",
        "    if not os.path.exists('gene_info.gz'):\n",
        "        print(\"Downloading gene info...\")\n",
        "        url = \"https://ftp.ncbi.nlm.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz\"\n",
        "        response = requests.get(url)\n",
        "        with open('gene_info.gz', 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "    # Read gene info file to get symbol to ID mapping\n",
        "    symbol_to_id = {}\n",
        "    with gzip.open('gene_info.gz', 'rt') as f:\n",
        "        next(f)  # Skip header\n",
        "        for line in f:\n",
        "            fields = line.strip().split('\\t')\n",
        "            gene_id = fields[1]\n",
        "            symbol = fields[2]\n",
        "            synonyms = fields[4].split('|')\n",
        "\n",
        "            # Map both main symbol and synonyms\n",
        "            symbol_to_id[symbol] = gene_id\n",
        "            for syn in synonyms:\n",
        "                if syn:\n",
        "                    symbol_to_id[syn] = gene_id\n",
        "\n",
        "    return symbol_to_id\n",
        "\n",
        "def create_go_features():\n",
        "    \"\"\"\n",
        "    Create GO features for perturbations with proper gene ID mapping\n",
        "    \"\"\"\n",
        "    # Load your dataset to get perturbation names\n",
        "    print(\"Loading dataset...\")\n",
        "    adata = sc.read(\"NormanWeissman2019_filtered_prepared.h5ad\")\n",
        "\n",
        "    # Get unique perturbations\n",
        "    pert1 = set(adata.obs['perturbation_1'].unique())\n",
        "    pert2 = set(adata.obs['perturbation_2'].unique())\n",
        "    all_perts = list(pert1.union(pert2) - {'control'})\n",
        "\n",
        "    print(f\"Total unique perturbations: {len(all_perts)}\")\n",
        "\n",
        "    # Get gene symbol to ID mapping\n",
        "    print(\"Getting gene ID mapping...\")\n",
        "    symbol_to_id = get_gene_id_mapping()\n",
        "\n",
        "    # Map our perturbation genes to IDs\n",
        "    pert_to_id = {}\n",
        "    unmapped_genes = []\n",
        "    for gene in all_perts:\n",
        "        if gene in symbol_to_id:\n",
        "            pert_to_id[gene] = symbol_to_id[gene]\n",
        "        else:\n",
        "            unmapped_genes.append(gene)\n",
        "\n",
        "    print(f\"Mapped {len(pert_to_id)} genes to NCBI IDs\")\n",
        "    print(f\"Unmapped genes: {len(unmapped_genes)}\")\n",
        "    if unmapped_genes:\n",
        "        print(\"First few unmapped genes:\", unmapped_genes[:5])\n",
        "\n",
        "    # Download files if they don't exist\n",
        "    if not os.path.exists('go.obo'):\n",
        "        !wget http://purl.obolibrary.org/obo/go.obo\n",
        "\n",
        "    if not os.path.exists('gene2go.gz'):\n",
        "        !wget https://ftp.ncbi.nlm.nih.gov/gene/DATA/gene2go.gz\n",
        "\n",
        "    # Create gene-GO associations\n",
        "    gene_to_go = {}\n",
        "    print(\"Reading gene2go file...\")\n",
        "    with gzip.open('gene2go.gz', 'rt') as f:\n",
        "        next(f)  # Skip header\n",
        "        for line in f:\n",
        "            fields = line.strip().split('\\t')\n",
        "            if len(fields) > 2:\n",
        "                tax_id = fields[0]\n",
        "                if tax_id == '9606':  # Human genes only\n",
        "                    gene_id = fields[1]\n",
        "                    go_id = fields[2]\n",
        "                    if gene_id in set(pert_to_id.values()):\n",
        "                        if gene_id not in gene_to_go:\n",
        "                            gene_to_go[gene_id] = set()\n",
        "                        gene_to_go[gene_id].add(go_id)\n",
        "\n",
        "    # Create feature matrix\n",
        "    print(\"Creating GO feature matrix...\")\n",
        "    all_go_terms = set()\n",
        "    for go_terms in gene_to_go.values():\n",
        "        all_go_terms.update(go_terms)\n",
        "\n",
        "    go_features = pd.DataFrame(0, index=all_perts, columns=list(all_go_terms))\n",
        "\n",
        "    for gene in all_perts:\n",
        "        if gene in pert_to_id:\n",
        "            gene_id = pert_to_id[gene]\n",
        "            if gene_id in gene_to_go:\n",
        "                for go_term in gene_to_go[gene_id]:\n",
        "                    go_features.loc[gene, go_term] = 1\n",
        "\n",
        "    # Save features\n",
        "    print(\"Saving GO features...\")\n",
        "    go_features.to_csv('go_features.csv')\n",
        "\n",
        "    return go_features\n",
        "\n",
        "# Run GO features creation\n",
        "go_features = create_go_features()"
      ],
      "metadata": {
        "id": "BEvh0MsZsXWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation with GO Features\n",
        "Incorporating GO features into the dataset"
      ],
      "metadata": {
        "id": "EiLqzwqjtXSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_data():\n",
        "    \"\"\"\n",
        "    Prepare training data with GO features\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    adata = sc.read(\"NormanWeissman2019_filtered_prepared.h5ad\")\n",
        "\n",
        "    # Load GO features\n",
        "    print(\"Loading GO features...\")\n",
        "    go_features = pd.read_csv('go_features.csv', index_col=0)\n",
        "\n",
        "    print(\"Creating cell-level GO features...\")\n",
        "    # Create GO feature matrix\n",
        "    go_matrix = np.zeros((adata.n_obs, go_features.shape[1]))\n",
        "\n",
        "    print(f\"Processing {adata.n_obs} cells...\")\n",
        "    for idx, row in enumerate(adata.obs.itertuples()):\n",
        "        if idx % 10000 == 0:\n",
        "            print(f\"Processed {idx} cells...\")\n",
        "\n",
        "        pert1 = row.perturbation_1\n",
        "        pert2 = row.perturbation_2\n",
        "\n",
        "        # Add GO features for both perturbations\n",
        "        if pert1 in go_features.index:\n",
        "            go_matrix[idx] += go_features.loc[pert1].values\n",
        "        if pert2 in go_features.index:\n",
        "            go_matrix[idx] += go_features.loc[pert2].values\n",
        "\n",
        "    # Add GO features to adata\n",
        "    print(\"Adding GO features to adata...\")\n",
        "    adata.obsm['go_features'] = go_matrix\n",
        "\n",
        "    # Save prepared data\n",
        "    print(\"Saving prepared data...\")\n",
        "    adata.write('data_with_go_features.h5ad')\n",
        "\n",
        "    print(\"Data preparation complete!\")\n",
        "    print(f\"Final data shape: {adata.shape}\")\n",
        "    print(f\"GO features shape: {adata.obsm['go_features'].shape}\")\n",
        "\n",
        "    return adata\n",
        "\n",
        "# Run data preparation\n",
        "adata = prepare_training_data()"
      ],
      "metadata": {
        "id": "z_8T23SRtZUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Pipeline\n",
        "Model configuration and training with both categorical perturbations and ordered GO features\n",
        "Note: Paths may need adjustment for HPC environment"
      ],
      "metadata": {
        "id": "grFostD4tvbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    \"\"\"\n",
        "    Train biolord model with GO features and perturbations\n",
        "    \"\"\"\n",
        "    # Load data in backed mode\n",
        "    print(\"Loading data in backed mode...\")\n",
        "    adata = sc.read('/content/drive/MyDrive/biolord_data/data_with_go_features.h5ad', backed='r')\n",
        "\n",
        "    # Get training data only\n",
        "    print(\"\\nPreparing training data...\")\n",
        "    train_mask = (adata.obs['partition'] == 'training')\n",
        "\n",
        "    # Create temporary filtered dataset\n",
        "    temp_filename = \"temp_train_data.h5ad\"\n",
        "    print(\"Creating filtered training dataset...\")\n",
        "    adata_train = sc.AnnData(\n",
        "        X=adata.X[train_mask],\n",
        "        obs=adata.obs[train_mask],\n",
        "        var=adata.var,\n",
        "        obsm={'go_features': adata.obsm['go_features'][train_mask]}\n",
        "    )\n",
        "\n",
        "    # Save filtered data\n",
        "    adata_train.write(temp_filename)\n",
        "    del adata  # Free memory\n",
        "    gc.collect()\n",
        "\n",
        "    # Load filtered data\n",
        "    print(\"Loading filtered data...\")\n",
        "    adata_train = sc.read(temp_filename)\n",
        "\n",
        "    # Setup model with both ordered and categorical attributes\n",
        "    print(\"\\nSetting up biolord...\")\n",
        "    biolord.Biolord.setup_anndata(\n",
        "        adata_train,\n",
        "        ordered_attributes_keys=['go_features'],\n",
        "        categorical_attributes_keys=['perturbation_1', 'perturbation_2']\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = biolord.Biolord(\n",
        "        adata_train,\n",
        "        n_latent=16,\n",
        "        module_params={\n",
        "            'decoder_width': 256,\n",
        "            'decoder_depth': 1,\n",
        "            'gene_likelihood': 'normal',\n",
        "            'reconstruction_penalty': 1e2,\n",
        "            'unknown_attribute_penalty': 1e1,\n",
        "            'n_latent_attribute_categorical': 16\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Training parameters\n",
        "    trainer_params = {\n",
        "        'decoder_lr': 1e-3,\n",
        "        'decoder_wd': 1e-4,\n",
        "        'attribute_nn_lr': 1e-2,\n",
        "        'attribute_nn_wd': 4e-8,\n",
        "        'step_size_lr': 45,\n",
        "        'cosine_scheduler': True,\n",
        "        'scheduler_final_lr': 1e-5,\n",
        "        'n_epochs_warmup': 0\n",
        "    }\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining model...\")\n",
        "    model.train(\n",
        "        max_epochs=5,\n",
        "        batch_size=1024,\n",
        "        early_stopping=False,\n",
        "        plan_kwargs=trainer_params,\n",
        "        enable_checkpointing=False,\n",
        "        enable_model_summary=False,\n",
        "        num_sanity_val_steps=0,\n",
        "        logger=False\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    save_path = \"/content/drive/MyDrive/biolord_go_model\"\n",
        "    print(f\"\\nSaving model to {save_path}...\")\n",
        "    model.save(save_path)\n",
        "\n",
        "    # Clean up\n",
        "    if os.path.exists(temp_filename):\n",
        "        os.remove(temp_filename)\n",
        "\n",
        "    print(\"Training complete and model saved!\")\n",
        "    return model\n",
        "\n",
        "# Run training\n",
        "model = train_model()"
      ],
      "metadata": {
        "id": "R_UcBLbxuNve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Code Verification\n",
        "Testing prediction code logic and structure before full implementation\n",
        "Note: This verifies the code structure without requiring full memory load"
      ],
      "metadata": {
        "id": "MztPHQCEus7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prediction_logic(n_test_perts=5):\n",
        "    \"\"\"\n",
        "    Test prediction code logic with minimal data processing\n",
        "    Just to verify the structure works\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Starting prediction logic test...\")\n",
        "        print(f\"Available RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
        "\n",
        "        # Load data references only (not full data)\n",
        "        print(\"\\nLoading data structure...\")\n",
        "        adata = sc.read('/content/drive/MyDrive/biolord_data/data_with_go_features.h5ad', backed='r')\n",
        "\n",
        "        # Get only perturbation information\n",
        "        test_mask = (adata.obs['partition'] == 'test')\n",
        "        test_perts = adata[test_mask].obs[['perturbation_1', 'perturbation_2']].drop_duplicates()\n",
        "        print(f\"\\nTotal test perturbation combinations: {len(test_perts)}\")\n",
        "        print(\"Sample combinations:\")\n",
        "        print(test_perts.head(n_test_perts))\n",
        "\n",
        "        # Check prediction structure\n",
        "        print(\"\\nChecking prediction structure...\")\n",
        "        print(\"Required attributes:\")\n",
        "        print(\"- Categorical attributes present:\", all(x in adata.obs.columns for x in ['perturbation_1', 'perturbation_2']))\n",
        "        print(\"- GO features present:\", 'go_features' in adata.obsm)\n",
        "        print(\"- Gene expression matrix shape:\", adata.shape)\n",
        "\n",
        "        # Verify model structure\n",
        "        print(\"\\nChecking model structure...\")\n",
        "        model_path = \"/content/drive/MyDrive/biolord_go_model/model.pt\"\n",
        "        if os.path.exists(model_path):\n",
        "            state_dict = torch.load(model_path, map_location='cpu')\n",
        "            if 'model_state_dict' in state_dict:\n",
        "                print(\"Model contains:\")\n",
        "                print(\"- latent_codes:\", 'latent_codes.embedding.weight' in state_dict['model_state_dict'])\n",
        "                print(\"- decoder:\", any('decoder' in k for k in state_dict['model_state_dict'].keys()))\n",
        "\n",
        "        print(\"\\nVerifying compute_prediction_adata requirements:\")\n",
        "        print(\"- Target attributes available:\", ['perturbation_1', 'perturbation_2'])\n",
        "        print(\"- Features for prediction:\", adata.var_names[:5], \"...\")\n",
        "\n",
        "        del adata\n",
        "        gc.collect()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in logic test: {str(e)}\")\n",
        "        print(\"\\nDetailed error information:\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Run test\n",
        "print(\"Starting prediction logic verification...\")\n",
        "success = test_prediction_logic()"
      ],
      "metadata": {
        "id": "jl10zTt1uuDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suggested Prediction Code for HPC Environment  \n",
        "This code reflects biolord's architecture requirements and handles:\n",
        "- Full training data loading for latent optimization\n",
        "- Both categorical perturbations and ordered GO features\n",
        "- Proper prediction structure based on biolord API\n",
        "\n",
        "Note: This code is for HPC execution where memory constraints are not an issue, it doesn't work on colab and has not been tested."
      ],
      "metadata": {
        "id": "09Gzlqaqu5Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions():\n",
        "    \"\"\"\n",
        "    Generate predictions using trained biolord model.\n",
        "\n",
        "    This code:\n",
        "    1. Properly handles full training data requirement for latent optimization\n",
        "    2. Manages both categorical (perturbations) and ordered (GO features) attributes\n",
        "    3. Uses biolord's compute_prediction_adata for predictions\n",
        "    4. Follows biolord's architecture requirements\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Starting prediction pipeline...\")\n",
        "\n",
        "        # Load full dataset (needed for latent optimization and reference)\n",
        "        print(\"\\nLoading data...\")\n",
        "        adata = sc.read('/content/drive/MyDrive/biolord_data/data_with_go_features.h5ad', backed='r')\n",
        "\n",
        "        # Get train/test data\n",
        "        print(\"\\nPreparing data splits...\")\n",
        "        train_mask = (adata.obs['partition'] == 'training')\n",
        "        adata_train = adata[train_mask].copy(filename=\"temp_train.h5ad\")\n",
        "        adata_train = sc.read(\"temp_train.h5ad\")\n",
        "\n",
        "        # Get initial states (control cells) from test set\n",
        "        print(\"\\nPreparing source data (initial states)...\")\n",
        "        source_mask = (adata.obs['partition'] == 'test') & (adata.obs['perturbation_1'] == 'control')\n",
        "        adata_source = adata[source_mask].copy(filename=\"temp_source.h5ad\")\n",
        "        adata_source = sc.read(\"temp_source.h5ad\")\n",
        "\n",
        "        print(\"\\nSetting up model...\")\n",
        "        biolord.Biolord.setup_anndata(\n",
        "            adata_train,\n",
        "            ordered_attributes_keys=['go_features'],\n",
        "            categorical_attributes_keys=['perturbation_1', 'perturbation_2']\n",
        "        )\n",
        "\n",
        "        print(\"\\nLoading model...\")\n",
        "        model_path = \"/content/drive/MyDrive/biolord_go_model\"\n",
        "        model = biolord.Biolord.load(model_path, adata=adata_train)\n",
        "\n",
        "        print(\"\\nAnalyzing test data...\")\n",
        "        test_perts = adata_source.obs[['perturbation_1', 'perturbation_2']].drop_duplicates()\n",
        "        print(f\"Found {len(test_perts)} unique perturbation combinations to predict\")\n",
        "\n",
        "        print(\"\\nGenerating predictions...\")\n",
        "        predictions = model.compute_prediction_adata(\n",
        "            adata=adata,                # Full data as reference\n",
        "            adata_source=adata_source,  # Test data as source\n",
        "            target_attributes=['perturbation_1', 'perturbation_2'],\n",
        "            add_attributes=['go_features']\n",
        "        )\n",
        "\n",
        "        print(\"\\nSaving predictions...\")\n",
        "        predictions.write('/content/drive/MyDrive/biolord_data/predictions.h5ad')\n",
        "\n",
        "        # Clean up\n",
        "        for f in [\"temp_train.h5ad\", \"temp_test.h5ad\"]:\n",
        "            if os.path.exists(f):\n",
        "                os.remove(f)\n",
        "\n",
        "        print(\"\\nPrediction pipeline complete!\")\n",
        "        print(f\"Predictions saved for {len(predictions)} cells\")\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError occurred: {str(e)}\")\n",
        "        print(\"\\nDetailed error information:\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Note: This code is designed for HPC execution\n",
        "print(\"Prediction code ready for HPC execution\")"
      ],
      "metadata": {
        "id": "dEtn2j_Q206X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suggested Prediction Code for HPC Environment with validation metrics\n",
        "This code reflects biolord's architecture requirements and includes:\n",
        "- Full training data loading for latent optimization\n",
        "- Both categorical perturbations and ordered GO features\n",
        "- Proper prediction structure based on biolord API\n",
        "- Validation metrics from biolord paper:\n",
        "  * R² score for prediction accuracy\n",
        "  * Normalized MSE for perturbation effects\n",
        "  * Gene-level correlation analysis"
      ],
      "metadata": {
        "id": "WvJYWUnj3MWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions():\n",
        "    \"\"\"\n",
        "    Generate and evaluate predictions using trained biolord model.\n",
        "\n",
        "    This code:\n",
        "    1. Properly handles full training data requirement for latent optimization\n",
        "    2. Manages both categorical (perturbations) and ordered (GO features) attributes\n",
        "    3. Uses biolord's compute_prediction_adata for predictions\n",
        "    4. Implements validation metrics from biolord paper\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Starting prediction pipeline...\")\n",
        "\n",
        "        # Load full dataset (needed for latent optimization and reference)\n",
        "        print(\"\\nLoading data...\")\n",
        "        adata = sc.read('/content/drive/MyDrive/biolord_data/data_with_go_features.h5ad', backed='r')\n",
        "\n",
        "        # Get train/test data\n",
        "        print(\"\\nPreparing data splits...\")\n",
        "        train_mask = (adata.obs['partition'] == 'training')\n",
        "        adata_train = adata[train_mask].copy(filename=\"temp_train.h5ad\")\n",
        "        adata_train = sc.read(\"temp_train.h5ad\")\n",
        "\n",
        "        # Get initial states (control cells) from test set\n",
        "        print(\"\\nPreparing source data (initial states)...\")\n",
        "        source_mask = (adata.obs['partition'] == 'test') & (adata.obs['perturbation_1'] == 'control')\n",
        "        adata_source = adata[source_mask].copy(filename=\"temp_source.h5ad\")\n",
        "        adata_source = sc.read(\"temp_source.h5ad\"))\n",
        "\n",
        "        print(\"\\nSetting up model...\")\n",
        "        biolord.Biolord.setup_anndata(\n",
        "            adata_train,\n",
        "            ordered_attributes_keys=['go_features'],\n",
        "            categorical_attributes_keys=['perturbation_1', 'perturbation_2']\n",
        "        )\n",
        "\n",
        "        print(\"\\nLoading model...\")\n",
        "        model_path = \"/content/drive/MyDrive/biolord_go_model\"\n",
        "        model = biolord.Biolord.load(model_path, adata=adata_train)\n",
        "\n",
        "        print(\"\\nAnalyzing test data...\")\n",
        "        test_perts = adata_source.obs[['perturbation_1', 'perturbation_2']].drop_duplicates()\n",
        "        print(f\"Found {len(test_perts)} unique perturbation combinations to predict\")\n",
        "\n",
        "        print(\"\\nGenerating predictions...\")\n",
        "        predictions = model.compute_prediction_adata(\n",
        "            adata=adata,                # Full data as reference\n",
        "            adata_source=adata_source,  # Test data as source\n",
        "            target_attributes=['perturbation_1', 'perturbation_2'],\n",
        "            add_attributes=['go_features']\n",
        "        )\n",
        "\n",
        "        print(\"\\nEvaluating predictions...\")\n",
        "        from scipy import stats\n",
        "        import numpy as np\n",
        "        from sklearn.metrics import r2_score\n",
        "\n",
        "        # Calculate metrics per perturbation combination\n",
        "        results = {}\n",
        "        for pert1 in test_perts['perturbation_1'].unique():\n",
        "            for pert2 in test_perts['perturbation_2'].unique():\n",
        "                # Get relevant cells\n",
        "                mask_pred = (predictions.obs['perturbation_1'] == pert1) & (predictions.obs['perturbation_2'] == pert2)\n",
        "                mask_true = (adata_source.obs['perturbation_1'] == pert1) & (adata_source.obs['perturbation_2'] == pert2)\n",
        "\n",
        "                if mask_pred.sum() > 0 and mask_true.sum() > 0:\n",
        "                    # Get expression values\n",
        "                    pred_exp = predictions[mask_pred].X\n",
        "                    true_exp = adata_source[mask_true].X\n",
        "\n",
        "                    # 1. R² score (from biolord paper)\n",
        "                    r2 = r2_score(true_exp, pred_exp)\n",
        "\n",
        "                    # 2. Normalized MSE (from biolord paper)\n",
        "                    control_exp = adata_source[adata_source.obs['perturbation_1'] == 'control'].X\n",
        "                    mse = np.mean((true_exp - pred_exp) ** 2)\n",
        "                    baseline_mse = np.mean((true_exp - np.mean(control_exp, axis=0)) ** 2)\n",
        "                    nmse = mse / baseline_mse if baseline_mse != 0 else float('inf')\n",
        "\n",
        "                    # 3. Pearson correlation per gene\n",
        "                    correlations = [stats.pearsonr(true_exp[:, i], pred_exp[:, i])[0]\n",
        "                                  for i in range(true_exp.shape[1])]\n",
        "                    mean_correlation = np.mean(correlations)\n",
        "\n",
        "                    results[(pert1, pert2)] = {\n",
        "                        'r2_score': r2,\n",
        "                        'normalized_mse': nmse,\n",
        "                        'mean_gene_correlation': mean_correlation\n",
        "                    }\n",
        "\n",
        "        # Calculate aggregate metrics\n",
        "        avg_metrics = {\n",
        "            'mean_r2': np.mean([v['r2_score'] for v in results.values()]),\n",
        "            'mean_nmse': np.mean([v['normalized_mse'] for v in results.values()]),\n",
        "            'mean_correlation': np.mean([v['mean_gene_correlation'] for v in results.values()])\n",
        "        }\n",
        "\n",
        "        print(\"\\nPrediction Results:\")\n",
        "        print(f\"Average R² score: {avg_metrics['mean_r2']:.3f}\")\n",
        "        print(f\"Average normalized MSE: {avg_metrics['mean_nmse']:.3f}\")\n",
        "        print(f\"Average gene correlation: {avg_metrics['mean_correlation']:.3f}\")\n",
        "\n",
        "        print(\"\\nSaving predictions and metrics...\")\n",
        "        predictions.uns['evaluation_metrics'] = {\n",
        "            'per_perturbation': results,\n",
        "            'aggregate_metrics': avg_metrics\n",
        "        }\n",
        "        predictions.write('/content/drive/MyDrive/biolord_data/predictions.h5ad')\n",
        "\n",
        "        # Clean up\n",
        "        for f in [\"temp_train.h5ad\", \"temp_test.h5ad\"]:\n",
        "            if os.path.exists(f):\n",
        "                os.remove(f)\n",
        "\n",
        "        print(\"\\nPrediction pipeline complete!\")\n",
        "        print(f\"Predictions saved for {len(predictions)} cells\")\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError occurred: {str(e)}\")\n",
        "        print(\"\\nDetailed error information:\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Note: This code is for HPC execution\n",
        "print(\"Prediction code ready for HPC execution\")"
      ],
      "metadata": {
        "id": "X0u8BdGF3Q7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}